{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Given this Youtube video transcript: 0:00\nIn this video, you're going to learn how to create Anki flashcards using ChatGPT.\n0:05\nOkay, so we're going to start by importing our dependencies. So essentially, I'm going to import\n0:09\nOS to access the OpenAI API key in my environment as my environment variable. I'm going to import\n0:15\nOpenAI clipboard to get the content from my clipboard. And then Json to save stuff as a Json.\n0:24\nHere's what we're going to do. First, we're going to set up an OpenAI API key.\n0:29\nThen we're going to get the content from my clipboard. So here I'm getting the content\n0:35\nfrom my clipboard. And the idea is I'm going to copy some text from whatever from a paper,\n0:41\nfrom a website, etc. And I'm going to run the script. And that's it. I'm going to copy some text,\n0:47\nrun the script. And when I run the script, it's going to access the content on my clipboard.\n0:51\nAnd from there is going to create the Enki flashcards by sending a request to the ChatGPT\n0:57\nAPI. So let's create a conversation with the ChatGPT. I'm going to set up the role for the\n1:06\nsystem as a helpful assistant. Before I had set up this as your helpful assistant that creates\n1:12\nAnki flashcards. But I found that it had just because of this little change in the system prompt,\n1:18\nit had problem creating Enki flashcards in the format that I wanted. And then I'm going to set\n1:25\nup the message for the user. And the message for the user is going to be something like create\n1:29\nEnki flashcards with the following text using a format like question answer, which is the format\n1:34\nthat's more easily importable to Enki. And I'm going to give it the clipboard content. And that's\n1:42\nit. That's the message. And now I can just set up the request to the API. So I'm just sending a\n1:49\nrequest to the check completion class, access the create method, and then saying, I'm specifying the\n1:55\nmodel to GPT for I'm specifying the messages that I just defined in this variable here, that are\n2:01\nhave the system message and the user message, the system message describing the behavior that I\n2:06\nexpect, and the user message describing what I want specifically, right, that's my actual prompt.\n2:12\nAnd then I'm going to set up the temperature to 0.7, which is like traditionally is a good number\n2:17\nto set for these kinds of things. I'm going to set up the max tokens. And I'm going to create,\n2:24\nI'm going to save the answer from the model to a variable called generate flashcards.\n2:30\nAnd from there, I'm going to save them to a file. And that's pretty much it. I actually\n2:37\ndidn't use Jason. So let's just remove this. And now let's test this out. So what we're going to do\n2:43\nis I'm going to have I have a paper here. And I'm just going to copy the contents from let's say,\n2:53\nthe beginning of this paper, let's say this was a paper that I was studying and I already read. And\n2:58\nnow I want to kind of like, okay, so I want the some information that's here, I want to save it.\n3:03\nBut I don't want to be creating each Enki flashcard, right? So I copied. So now let's check that I\n3:09\ncopied. Perfect. It's in my clipboard. So now I can just come here and run the script. So I'm\n3:15\ngoing to run flashcards, check it to be perfect. And now let's see the magic happen. So I run the\n3:23\nscript, it takes like a few seconds to finish sending the request and getting the response.\n3:30\nAll right, perfect. So now I finished running. So I can access that file. And as you can see,\n3:37\nit created the flashcards. So what is camel camel stands for communicative agents for mind\n3:43\nexploration. What is the goal of camel? What is role playing framework? What is inception prompting?\n3:50\nAll useful information that was present in the text. Now I can go to Enki.\n3:55\nSo I can go to Enki right here. And I can import those flashcards directly. You see, it created\n4:03\nthe flashcards front, back, everything is correct. And I can just click import. And now I have the\n4:09\nflashcards to train to access and etc. So yeah, this approach is really cool. I really like this.\n4:16\nI like I really like doing this because you know, you can also do it directly with the\n4:22\ncharge PT demo or the charge PT website, which also works really well. But having it as a script\n4:29\nthat copies, I can just, you know, stop not think about it too much and just have it on my clipboard\n4:36\nand then just run the script. And I can automate the running of the script. So I can just do it\n4:41\nwith a few commands. And I have a bunch of Enki flashcards that usually would have taken me\n4:46\nquite a few minutes to create. So if you like this video, don't forget to like and subscribe\n4:52\nand see you next time. Cheers. I want you to create a chapters section for this Youtube video with the following format:\n\ud83d\udcda Chapters:\n<double digit:time stamp> - <Concise phrase of a major part of the video>"}, {"role": "assistant", "content": "\ud83d\udcda Chapters:\n00:00 - Introduction \n00:01 - Creating Anki Flashcards with ChatGPT\n00:06 - Importing Dependencies\n00:14 - Setting Up OpenAI API Key\n00:29 - Getting Content from Clipboard\n00:46 - Running the Script\n00:57 - Creating Conversation with ChatGPT\n01:05 - Setting Up System Role\n01:24 - Setting Up User Message\n01:38 - Formatting for Anki Import\n01:46 - Sending Request to API\n02:35 - Writing the flashcards to file\n02:50 - Creating Anki flashcards\n03:58 - Importing flashcards to Anki"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Given this Youtube video transcript: 0:00\nin this video you're going to learn how to build  a simple PDF summarization app okay to get started  \n0:05\nlet's set up our Imports so first we're going  to import gradu to have the app which is going  \n0:12\nto be the framework that we're going to use  to actually build the actual app then we're  \n0:16\ngoing to use Link Chain to do the summarization  stuff here I'm importing open AI to access the  \n0:21\nlarge language model and the prompt template  to do custom prompts for the summarization  \n0:27\nthen we're going to set up a character we're  going to import character splitter which we're  \n0:31\ngoing to use to split the text document into a  PDF into text chunks then we're going to load  \n0:39\nthe summarization chain from link chain and the  pi PDF loader to load the PDF so now I'm going  \n0:46\nto be setting up the large language model from  an open AI this is essentially charger PT and  \n0:51\nI'm setting temperature to 0 to be as precise as  possible and I'm setting up the text splitter to  \n0:58\nsplit the document into chunks now we're going  to do two functions for this one to summarize  \n1:03\nthe PDF and one to do custom summaries so first  we're going to be setting up the summarize PDF  \n1:09\nfunction which is going to take in the PDF file  path and a custom prompt which is going to be an  \n1:15\nempty string if I'm not doing a custom summary  but if I'm doing a custom summary this is going  \n1:20\nto carry that custom prompt then inside this  function I'm going to load the PDF using the pi  \n1:27\nPDF loader class then I'm going to load and split  the documents that I just I loaded the PDF with  \n1:35\nloader now I'm splitting that into chunks and I'm  going to save that to a variable called Docs then  \n1:40\nI'm going to set up a summarization chain with  the load some rise chain function from link chain  \n1:46\nand I'm going to set the Ching type to map  reduce there are many ways to do this there's  \n1:52\nlike the refine method there are a few chain  types that you can experiment with but I found  \n1:56\nthat this one works pretty well and then we're  going to run our summarization on the document  \n2:02\nand here I'm setting up a simple if where  if the custom prompt is not an empty string  \n2:11\nI'm going to do a custom summarization so that  means what that means that I'm gonna get the  \n2:18\ncustom prompt from the keyword argument in the  function and I'm going to build my prompt template  \n2:26\nusing the prompt template class from link chain  I'm fitting in the prompt template that I built  \n2:33\nusing the custom proc that I fed to this function  and I'm setting input variable Stacks which are  \n2:39\ngoing to be the uh essentially is what this  function is going to be taking in as the custom  \n2:45\nprompt and placing it um placing it as the text to  summarize then I'm going to summarize the content  \n2:55\nand in the load summarization chain I'm going  to map The Prompt that I just built to that low  \n3:03\nsummarization chain function so this variable  goes into here so that this thing understands  \n3:08\nthat it has to do a custom summary then I'm just  gonna run my custom summarization and that's it  \n3:17\nfinally here if there's no custom prompt I'm  just gonna set custom summary as an empty  \n3:26\nstring because this function returns both  a regular summary and a customized summary  \n3:32\nnow essentially before I had this as a separate  function like the custom summary like this but I  \n3:40\ndidn't really like this approach so although it's  here we're not going to be using this function  \n3:47\nand finally what I'm going to be doing is I'm  going to define a main function where we're  \n3:51\ngoing to actually build the app which will have  as a backend this function that's going to be  \n3:56\ndoing all the summarization for us so to build  the actual app we have to set up our inputs and  \n4:01\noutputs so the input is going to be the PDF  file path as a string so I'm going to feed it  \n4:07\nto a text box in gradu then I'm going to set up  an input for the custom prompt which is going to  \n4:14\nbe the same thing a text boxing radio and then I'm  going to set up the output as the output summary  \n4:20\nvariable which is also going to be a text box  where it's going to contain the actual summary  \n4:26\nand then an output custom summary which um  I think I'm not going to be using this one  \n4:34\nyeah so I did this a few versions and now I'm  just going through the line by line with the  \n4:40\nwith the code that I built before but [Music] um  now what I'm doing is I'm setting up a nice face  \n4:47\nvariable which um takes in a radio interface  class and then it feeds the function summarize  \n4:55\nPDF which is the function that we built here  then it's setting up inputs and outputs so as  \n5:01\ninputs it's a list with the input to the PDF  file path and an input to the custom prompt  \n5:07\nwhich is a just the text defining the custom  prompt that we want and as an output we have  \n5:13\noutput summary and output custom summary which  are right here they're going to be the text boxes  \n5:22\nthat are going to be storing the outputs from the  summarize PDF remember that summarize PDF function  \n5:28\nis returning a regular summary and a custom  summary if the custom prompt is fed to this  \n5:34\nfunction custom prompt I'm sure there are better  ways to do it but this was the the way that I  \n5:40\nfound worked the best then I'm going to launch my  app and that's pretty much it now I'm just saying  \n5:48\num now I'm just going to run the main function and  let's test this out so I already I already tested  \n5:55\nbefore so it's already working here but let's test  it again and see if everything is working great\n6:03\nokay so now I click the link with the URL  okay that looks good now what we're going  \n6:10\nto be doing is I'm going to get a path for a  PDF this is something that before people had  \n6:16\nstruggled with regarding the finding  the path to the PDF so I'm just gonna\n6:25\nseed PDF file path right here and then I'm  not going to give a custom prompt I'm just  \n6:32\ngoing to submit this file path and now it's  running so it's going to take a few seconds  \n6:36\nand once I think usually in my machine which is  not exactly a super amazing machine it takes a  \n6:44\nlittle bit a little while but boom there we  go this paper introduces video Transformer  \n6:51\npre-trained video model that captures semantics  and structure of instructional videos it's  \n6:54\nevaluated on six Downstream tasks including  step classification mistake step detection  \n6:59\netc etc now we can cross check with the actual  paper and this is the paper learning and  \n7:05\nverification of test structure instructional  videos video task former is the name of the  \n7:11\nmodel that they developed so this summary  is perfectly aligned with the actual paper  \n7:16\nand that's it that's how you build a very simple  PDF serverization app using radio and linking if  \n7:24\nyou like this video don't forget to like  And subscribe and see you next time cheers I want you to create a chapters section for this Youtube video with the following format:\n\ud83d\udcda Chapters:\n<double digit:time stamp> - <Concise phrase of a major part of the video>"}, {"role": "assistant", "content": "\ud83d\udcda Chapters\n00:01 - Introduction to the PDF Summarization App\n00:30 - Setting up Imports and Loading Language Model\n01:03 - Creating Functions for Summarizing PDFs\n03:27 - Defining the Main Function for Building the App\n04:02 - Setting up Inputs and Outputs in the Gradio Interface\n05:13 - Launching the App and Testing\n07:05 - Conclusion and Next Steps"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Given this Youtube video transcript: Introduction\n0:00\ntoday let's build another transcription\nBuilding the transcription app with Whisper & Gradio\n0:02\napp using whisper and gradio\n0:04\nso a while back I did a video on\n0:07\nbuilding another transcription app using\n0:09\nwhisper and streamlit and it got great\n0:11\nresponse and a lot of comments which was\n0:13\nreally great and so I decided to do a\n0:17\nsecond video on the same topic using\n0:19\nradio this time to make it a little bit\n0:21\nsimpler and easier to run also I\n0:24\npromised someone in the comments to do\n0:25\nthat so I always try to keep my promises\n0:27\nokay so we're going to start by simply\n0:29\nimporting the dependencies that we need\nImporting dependencies\n0:31\nso we're going to import radio as gr and\n0:34\nwe're going to import whisper for doing\n0:37\nthe audio transcription now the first\n0:39\nthing we're going to do is we're going\n0:40\nto write a function called transcribe\nWriting the transcribe_audio function\n0:43\naudio which will take in a file path and\n0:47\noutput the transcription of that file\n0:49\nthe audio file path then we're going to\n0:52\nset up the model from whisper by just\n0:54\ndoing\n0:55\nwhisper.load model and we're going to\n0:58\nuse the base model because you know it\n1:01\nworks really well for English and now\n1:03\nwe're gonna just set up the result as\n1:06\nthe calling the model.transcribe\n1:09\nfunction from The Whisper model that we\n1:11\nloaded into that transcribed function\n1:13\nwe're going to feed the audio file which\n1:17\nis just going to be the file path to\n1:18\nthat audio and finally we're going to\n1:20\nreturn the transcription so that's easy\n1:23\nenough\n1:24\nnow we're going to set up our main\nSetting up the main function\n1:26\nfunction\n1:27\nand the main function is going to be for\n1:29\nbuilding the interfacing gradient and\n1:32\nthat will take in a nodule input which\n1:34\nis going to be gradual input audio\n1:39\nwhich is something that we can just drag\n1:41\nand drop an audio file or we can just\n1:43\nclick it and then upload from our local\n1:44\ndesktop the source is going to be called\n1:48\nupload and the type is going to be\n1:51\ncalled file path because that's what\n1:54\nthis function accepts in order to run\n1:57\nand now we're going to say output text\nDefining the Gradio interface\n2:00\nis equal to gr dot\n2:05\noutputs.txt box in white Xbox because\n2:08\nit's the text box that's going to be\n2:11\ncarrying it's going to be having the um\n2:13\nit's going to have the transcription of\n2:15\nthe audio file path\n2:17\nfinally once we've set that up we're\n2:19\njust going to set up our interface as\n2:22\nradio dot interface and then we're going\n2:26\nto say that the function for our\n2:28\ninterface is the transcribe audio\n2:31\nfunction\n2:32\nwhich is the function we just defined\n2:34\nover there\n2:36\nnow we're going to set up the inputs as\n2:39\nwe're going to set up the inputs as the\n2:42\naudio input right and we're going to set\n2:45\nup the outputs as the output that we\n2:48\njust Define here this variable that\n2:51\ncarries the output from the text box\n2:54\nokay that's perfect so now what we can\n2:57\ndo is\n2:58\nI can come here and I can say\nAdding a title & description for the app\n3:02\num let's set up a title for our app it's\n3:06\njust going to be audio transcription\n3:10\napp because you know why not and a quick\n3:15\ndescription is just going to be\n3:17\nupload an audio file\n3:20\nand hit the\n3:24\nsubmit button\n3:28\nthere we go\n3:30\nperfect now we got it nice let's have\n3:35\nthis thing over here perfect\n3:37\nnow that we have that we have to launch\n3:39\nour interface so we're going to just\n3:41\ncall lunch\nLaunching the interface\n3:42\nand that's our app in a nutshell because\n3:45\npython is amazing and we can build super\n3:48\ncomplex and interesting stuff with just\n3:50\na few lines of code and now I can just\n3:53\nsay if name equal main I'm going to call\n3:56\nthe main function which is going to set\n3:59\nup my\n4:00\ntranscription or the transcription app\n4:03\nand now that I have all of that we can\nTesting the app\n4:05\ntest it so I can come here and say\n4:07\npython Auto transcription app I already\n4:10\nhave all radio and Whisper installed so\n4:12\nif you don't run pip install radio\n4:14\nwhisper Etc\n4:16\nso now we're running the audio\n4:18\ntranscription app which should open here\n4:20\non the browser on my right\n4:22\nso let's take a look at that\n4:24\nwe'll click on the link that's provided\n4:26\nand now this is our Simple app\n4:29\nand I'm going to click here on the Audio\n4:31\nI already have an audio file set up we\n4:34\ncan listen to it\n4:39\nawesome and now I can hit submit and I\n4:42\nwill do a magic trick and there we have\n4:45\nit here's our transcription for the\n4:48\naudio input that we set up I'm going to\n4:50\nset up a nicely organized GitHub repo\n4:54\nwith this project so that you can run it\n4:56\njust by running one command and I want\n4:59\nto make it easy for you guys because you\n5:01\nguys gave me a lot of cool input and a\n5:03\nbunch of awesome comments for the last\n5:05\nvideo that I did on audio transcription\n5:07\nso I wanted to do that again in a\n5:09\nsimpler way and make it easy so that for\n5:12\nthose of you that want to use it you can\n5:14\njust go to my repo and just clone and\n5:18\nrun and so thanks for watching guys if\nConclusion and follow up\n5:20\nyou like this video don't forget to like\n5:22\nAnd subscribe and see you next time\n5:24\ncheers I want you to create a chapters section for this Youtube video with the following format:\n\ud83d\udcda Chapters:\n<double digit:time stamp> - <Concise phrase of a major part of the video>"}, {"role": "assistant", "content": "\ud83d\udcdaChapters\n0:00 - Introduction\n0:02 - Building the transcription app with Whisper & Gradio\n0:31 - Importing dependencies\n0:43 - Writing the transcribe_audio function\n1:26 - Setting up the main function\n2:00 - Defining the Gradio interface\n3:02 - Adding a title & description for the app\n3:42 - Launching the interface\n4:05 - Testing the app\n5:20 - Conclusion and follow up"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Given this Youtube video transcript: Intro\n0:00\nall right so today we're going to be\n0:01\nbuilding the automation for daily\n0:03\nscheduling so scheduling your to-do list\n0:05\nfor today essentially the idea is to\n0:07\ngrab a bunch of tasks from a database of\n0:10\ntasks that you have to do and then you\n0:12\nknow filter them according to some\n0:13\nparameters and create a tailored\n0:17\nschedule for that day and the tasks that\n0:20\nyou have to do sorry about my voice but\n0:22\nI was cheering for a friend of mine who\n0:24\nwent to fight in an MMA event and now I\n0:27\nlost my voice so it is what it is so\n0:29\nlet's get started so for the first task\n0:31\nof automating daily scheduling we're\nTasks Database\n0:33\ngoing to be based on this idea of a task\n0:37\ndatabase where I have a test database\n0:39\ncomposed of a task a type content\n0:43\ncreation personal work learning Etc it\n0:45\ndid an effort which is an estimation of\n0:48\nthe number of hours that takes to\n0:49\ncomplete a task then a date which is a\n0:52\ndeadline for the task if the task has a\n0:55\ndeadline finally the task the database\n0:58\nwill also have a priority column\n1:00\nindicating the level of importance of\n1:02\nthe task where one means very low\nImports\n1:05\npriority and five means high priority\n1:07\nokay perfect okay guys so first things\n1:10\nfirst we're going to import our\n1:11\ndependencies so we're going to be\n1:12\nimporting open AI to access the\n1:14\nchargeable TPI then we're going to have\n1:16\na few other Imports to handle some\nSetting up the ChatGPT API\n1:17\nprocessing then what we're going to do\n1:19\nis we're going to first Define the get\n1:21\nresponse function to access the\n1:22\nchargeable TPI so here we're essentially\n1:24\njust using the open AI API and then the\n1:26\ncheck completion class and the create\n1:28\nmethod to access in this case we're\n1:30\ngoing to be accessing GPT 3.5 turbo but\n1:34\nI actually want to test with both models\n1:36\nto see which one works the best then\n1:37\nwe're going to set up the message for\n1:40\nthe system and in this case I'm just\n1:42\nsaying you're helpful research and\n1:44\nprogramming assistant and then I will\n1:47\nsend the prompt question which will be\n1:49\nfed to the function as an argument then\n1:51\nI'm going to respond return the response\nSetting the OpenAI API key\n1:53\nof the model so pretty boilerplate and\n1:55\nI'm going to set up the open air API key\n1:57\nthen I'm going to read a CSV file with\n1:59\nthe database that I exported from notion\nLoading Tasks DB from csv file\n2:02\nso essentially I'm exporting this\n2:04\ndatabase to a CSV file then I unzip it\n2:07\nand save it to a file called Tesla CSV\n2:11\nfinally I'm gonna get today's date which\n2:14\nI'm going to be using to feed the model\n2:17\nwith the information of which day is\n2:18\ntoday so that it can do proper\n2:20\nscheduling of the tasks according to the\n2:22\nday and then I'm going to transform that\n2:25\ntask to a string because here this is an\n2:29\ninteresting part I'm not going to be\n2:30\nusing link chain or I'm not going to be\n2:33\nvectorizing the test database I wanted\n2:36\nto do something a little bit simpler and\n2:38\nso I'm just going to feed the string\n2:40\nbecause the string is not going to be\n2:41\nthat grip that big with this amount of\n2:44\ninformation because I don't want to do\n2:45\nlike a super planning of like a month I\n2:48\njust want to do like a basic\n2:49\ndaily planning of scheduling some tasks\nMain prompt for ChatGPT\n2:53\nfinally we're going to Define our prompt\n2:55\nnow here is where it gets interesting so\n2:58\nthe prompt is going to be consider that\n3:00\ntoday is someday X then write a test\n3:03\nschedule for tomorrow consider the\n3:06\nfollowing effort column represents\n3:08\nprojection of hours to complete the task\n3:11\nthe goal should be to focus more on work\n3:13\nand content creation but do other tasks\n3:16\nif there is time respect to priority\n3:18\nlevels and make this schedule as\n3:19\noptimized as possible tasks with\n3:22\ndeadlines should be performed before or\n3:23\nat the day of the deadline and the\n3:25\noutperformance should be start time and\n3:27\ntime task and the task database is this\n3:30\nso essentially I give as much context as\n3:33\nI can to the model\nWriting Output to file\n3:34\nand that I print the result of the term\n3:36\nLoop\n3:37\nand that's pretty much it I also want to\n3:40\nsave it as a schedule so let's just save\n3:44\nit as daily schedule.xc\n3:49\nI'm going to access and then put\n3:52\nnow we're going to say after right and\n3:55\nthen I'm going to say\n3:57\nah\n3:58\nwe're going to take this as out of the\n4:00\nmodel so I'll put output schedule secret\n4:04\nto this and then we're going to print it\n4:07\nand we're also going to write a file\n4:09\nbeautiful and that's pretty much it so\n4:11\nlet's give it a test run to this version\n4:14\nof the automation automating the\n4:16\nschedule let's take a look alright cool\n4:18\nso we got our response back\nVisualizing the task schedule\n4:21\nso let's take a look let's save this\n4:23\nactually I'm gonna rename this as daily\n4:26\nschedule\n4:27\ngp3 uh 3.5 turbo so this is the output\n4:31\nthat I got so 9 to 10 30 finish\n4:33\nremaining minor that's work okay that\n4:35\nmakes sense this is a work task okay 12\n4:38\nto 1 say your health Matrix I'll cry\n4:41\nI'll cry and then to five or last up\n4:43\nwords yeah it had all sorts of tests in\n4:44\nthe database note that the priority\n4:46\nlevels have been considered in all the\n4:48\ntasks have been scheduled okay so this\n4:50\nlooks pretty good but now what I would\nComparing with GPT-4\n4:51\nlike to do is compare that to be using\n4:54\ngpt4 so I'm Gonna Save the same thing\n4:57\nand I'm gonna say with gpt4 and I'm\n5:01\ngonna run this again so I'm going to\n5:03\nsave this and run it again let's take a\n5:05\nlook to see what it looks like all right\n5:06\nperfect so we got a response and this is\n5:10\nactually looking pretty good so we have\n5:13\nthis format that I asked for the only\n5:15\nthing the model got around was the date\n5:17\nbut that's my mistake because I fed it\n5:19\nin the wrong format yeah I should have\n5:23\nsaid month should have said month day\n5:26\nyear that's why we can assess the output\n5:29\nrecord the short Monday Tuesday video\n5:31\nand then there's some learning tasks\n5:33\nokay I mean the model is assuming that\n5:35\nI'm going to be working from 8 AM to 10\n5:37\no'clock but that's okay\n5:39\nand now we can take a look at both\n5:42\nschedules side by side so one thing that\n5:46\nI notice I like the concisiveness the\n5:48\ncon how concise the output from gpd4 is\n5:51\nso I like that\n5:54\num\n5:55\nI feel like in terms of for example work\n5:59\npresentation is a one hour task in\n6:02\nYouTube before said that it would take\n6:05\ntwo hours while let's see well this was\n6:09\na task that actually gbt 3.5 didn't even\n6:12\nconsider now one thing I didn't like was\n6:15\nthat the um\n6:17\npriorities don't seem to be that\n6:19\nstraight because\n6:21\nthe most important task was\n6:25\nuh the most important task was\n6:29\nhere exactly\n6:30\nso there was this one that was a big\n6:32\ntask\n6:34\nwhich was to do some modifications on an\n6:37\nAPI thing that I have to do for work\n6:39\nand then finishing build changes then\n6:42\nfinishing remaining minor tasks\n6:44\nbut it's okay I mean the the fact that I\n6:47\nhad an effort of 12 hours for the big\n6:49\ntask makes sense that it would not\n6:51\nschedule when I don't have the time to\n6:53\ncomplete it so I would say that this is\n6:55\na pretty cool application of charger PT\n6:58\nand using the API I think that one of\nFinal Thoughts\n7:00\nthe ideas that I had was to connect this\n7:01\nwith notion and I bet that you can do\n7:03\nthat with zapier and stuff but I can\n7:05\nalso do it with python code which is\n7:07\ngreat because then you can have this\n7:09\nrunning and then you just you know\n7:11\ncreate a schedule for the day and you\n7:13\njust go through the day and you don't\n7:14\nhave to think about it so this is a\n7:16\npretty good example of an application\n7:18\nusing charge BT that's it hope you guys\n7:20\nlike this video if you like it don't\n7:21\nforget to like And subscribe and see you\n7:23\nnext time cheers I want you to create a chapters section for this Youtube video with the following format:\n\ud83d\udcda Chapters:\n<double digit:time stamp> - <Concise phrase of a major part of the video>"}, {"role": "assistant", "content": "\ud83d\udcda Chapters:\n00:00 - Intro\n00:32 - Tasks Database\n01:05 - Imports\n01:17 - Setting up the ChatGPT API\n01:52 - Setting the OpenAI API key\n02:00 - Loading Tasks DB from csv file\n02:51 - Main prompt for ChatGPT\n03:34 - Writing Output to file\n04:19 - Visualizing the task schedule\n04:51 - Comparing with GPT-4\n07:00 - Final Thoughts"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Given this Youtube video transcript: Intro\n0:00\nall right so today we're going to be\n0:01\nbuilding the automation for daily\n0:03\nscheduling so scheduling your to-do list\n0:05\nfor today essentially the idea is to\n0:07\ngrab a bunch of tasks from a database of\n0:10\ntasks that you have to do and then you\n0:12\nknow filter them according to some\n0:13\nparameters and create a tailored\n0:17\nschedule for that day and the tasks that\n0:20\nyou have to do sorry about my voice but\n0:22\nI was cheering for a friend of mine who\n0:24\nwent to fight in an MMA event and now I\n0:27\nlost my voice so it is what it is so\n0:29\nlet's get started so for the first task\n0:31\nof automating daily scheduling we're\nTasks Database\n0:33\ngoing to be based on this idea of a task\n0:37\ndatabase where I have a test database\n0:39\ncomposed of a task a type content\n0:43\ncreation personal work learning Etc it\n0:45\ndid an effort which is an estimation of\n0:48\nthe number of hours that takes to\n0:49\ncomplete a task then a date which is a\n0:52\ndeadline for the task if the task has a\n0:55\ndeadline finally the task the database\n0:58\nwill also have a priority column\n1:00\nindicating the level of importance of\n1:02\nthe task where one means very low\nImports\n1:05\npriority and five means high priority\n1:07\nokay perfect okay guys so first things\n1:10\nfirst we're going to import our\n1:11\ndependencies so we're going to be\n1:12\nimporting open AI to access the\n1:14\nchargeable TPI then we're going to have\n1:16\na few other Imports to handle some\nSetting up the ChatGPT API\n1:17\nprocessing then what we're going to do\n1:19\nis we're going to first Define the get\n1:21\nresponse function to access the\n1:22\nchargeable TPI so here we're essentially\n1:24\njust using the open AI API and then the\n1:26\ncheck completion class and the create\n1:28\nmethod to access in this case we're\n1:30\ngoing to be accessing GPT 3.5 turbo but\n1:34\nI actually want to test with both models\n1:36\nto see which one works the best then\n1:37\nwe're going to set up the message for\n1:40\nthe system and in this case I'm just\n1:42\nsaying you're helpful research and\n1:44\nprogramming assistant and then I will\n1:47\nsend the prompt question which will be\n1:49\nfed to the function as an argument then\n1:51\nI'm going to respond return the response\nSetting the OpenAI API key\n1:53\nof the model so pretty boilerplate and\n1:55\nI'm going to set up the open air API key\n1:57\nthen I'm going to read a CSV file with\n1:59\nthe database that I exported from notion\nLoading Tasks DB from csv file\n2:02\nso essentially I'm exporting this\n2:04\ndatabase to a CSV file then I unzip it\n2:07\nand save it to a file called Tesla CSV\n2:11\nfinally I'm gonna get today's date which\n2:14\nI'm going to be using to feed the model\n2:17\nwith the information of which day is\n2:18\ntoday so that it can do proper\n2:20\nscheduling of the tasks according to the\n2:22\nday and then I'm going to transform that\n2:25\ntask to a string because here this is an\n2:29\ninteresting part I'm not going to be\n2:30\nusing link chain or I'm not going to be\n2:33\nvectorizing the test database I wanted\n2:36\nto do something a little bit simpler and\n2:38\nso I'm just going to feed the string\n2:40\nbecause the string is not going to be\n2:41\nthat grip that big with this amount of\n2:44\ninformation because I don't want to do\n2:45\nlike a super planning of like a month I\n2:48\njust want to do like a basic\n2:49\ndaily planning of scheduling some tasks\nMain prompt for ChatGPT\n2:53\nfinally we're going to Define our prompt\n2:55\nnow here is where it gets interesting so\n2:58\nthe prompt is going to be consider that\n3:00\ntoday is someday X then write a test\n3:03\nschedule for tomorrow consider the\n3:06\nfollowing effort column represents\n3:08\nprojection of hours to complete the task\n3:11\nthe goal should be to focus more on work\n3:13\nand content creation but do other tasks\n3:16\nif there is time respect to priority\n3:18\nlevels and make this schedule as\n3:19\noptimized as possible tasks with\n3:22\ndeadlines should be performed before or\n3:23\nat the day of the deadline and the\n3:25\noutperformance should be start time and\n3:27\ntime task and the task database is this\n3:30\nso essentially I give as much context as\n3:33\nI can to the model\nWriting Output to file\n3:34\nand that I print the result of the term\n3:36\nLoop\n3:37\nand that's pretty much it I also want to\n3:40\nsave it as a schedule so let's just save\n3:44\nit as daily schedule.xc\n3:49\nI'm going to access and then put\n3:52\nnow we're going to say after right and\n3:55\nthen I'm going to say\n3:57\nah\n3:58\nwe're going to take this as out of the\n4:00\nmodel so I'll put output schedule secret\n4:04\nto this and then we're going to print it\n4:07\nand we're also going to write a file\n4:09\nbeautiful and that's pretty much it so\n4:11\nlet's give it a test run to this version\n4:14\nof the automation automating the\n4:16\nschedule let's take a look alright cool\n4:18\nso we got our response back\nVisualizing the task schedule\n4:21\nso let's take a look let's save this\n4:23\nactually I'm gonna rename this as daily\n4:26\nschedule\n4:27\ngp3 uh 3.5 turbo so this is the output\n4:31\nthat I got so 9 to 10 30 finish\n4:33\nremaining minor that's work okay that\n4:35\nmakes sense this is a work task okay 12\n4:38\nto 1 say your health Matrix I'll cry\n4:41\nI'll cry and then to five or last up\n4:43\nwords yeah it had all sorts of tests in\n4:44\nthe database note that the priority\n4:46\nlevels have been considered in all the\n4:48\ntasks have been scheduled okay so this\n4:50\nlooks pretty good but now what I would\nComparing with GPT-4\n4:51\nlike to do is compare that to be using\n4:54\ngpt4 so I'm Gonna Save the same thing\n4:57\nand I'm gonna say with gpt4 and I'm\n5:01\ngonna run this again so I'm going to\n5:03\nsave this and run it again let's take a\n5:05\nlook to see what it looks like all right\n5:06\nperfect so we got a response and this is\n5:10\nactually looking pretty good so we have\n5:13\nthis format that I asked for the only\n5:15\nthing the model got around was the date\n5:17\nbut that's my mistake because I fed it\n5:19\nin the wrong format yeah I should have\n5:23\nsaid month should have said month day\n5:26\nyear that's why we can assess the output\n5:29\nrecord the short Monday Tuesday video\n5:31\nand then there's some learning tasks\n5:33\nokay I mean the model is assuming that\n5:35\nI'm going to be working from 8 AM to 10\n5:37\no'clock but that's okay\n5:39\nand now we can take a look at both\n5:42\nschedules side by side so one thing that\n5:46\nI notice I like the concisiveness the\n5:48\ncon how concise the output from gpd4 is\n5:51\nso I like that\n5:54\num\n5:55\nI feel like in terms of for example work\n5:59\npresentation is a one hour task in\n6:02\nYouTube before said that it would take\n6:05\ntwo hours while let's see well this was\n6:09\na task that actually gbt 3.5 didn't even\n6:12\nconsider now one thing I didn't like was\n6:15\nthat the um\n6:17\npriorities don't seem to be that\n6:19\nstraight because\n6:21\nthe most important task was\n6:25\nuh the most important task was\n6:29\nhere exactly\n6:30\nso there was this one that was a big\n6:32\ntask\n6:34\nwhich was to do some modifications on an\n6:37\nAPI thing that I have to do for work\n6:39\nand then finishing build changes then\n6:42\nfinishing remaining minor tasks\n6:44\nbut it's okay I mean the the fact that I\n6:47\nhad an effort of 12 hours for the big\n6:49\ntask makes sense that it would not\n6:51\nschedule when I don't have the time to\n6:53\ncomplete it so I would say that this is\n6:55\na pretty cool application of charger PT\n6:58\nand using the API I think that one of\nFinal Thoughts\n7:00\nthe ideas that I had was to connect this\n7:01\nwith notion and I bet that you can do\n7:03\nthat with zapier and stuff but I can\n7:05\nalso do it with python code which is\n7:07\ngreat because then you can have this\n7:09\nrunning and then you just you know\n7:11\ncreate a schedule for the day and you\n7:13\njust go through the day and you don't\n7:14\nhave to think about it so this is a\n7:16\npretty good example of an application\n7:18\nusing charge BT that's it hope you guys\n7:20\nlike this video if you like it don't\n7:21\nforget to like And subscribe and see you\n7:23\nnext time cheers I want you to create a chapters section for this Youtube video with the following format:\n\ud83d\udcda Chapters:\n<double digit:time stamp> - <Concise phrase of a major part of the video>"}, {"role": "assistant", "content": "\ud83d\udcda Chapters:\n\n00:00 - Intro\n00:32 - Tasks Database\n01:05 - Imports\n01:17 - Setting up the ChatGPT API\n01:52 - Setting the OpenAI API key\n02:00 - Loading Tasks DB from csv file\n02:51 - Main prompt for ChatGPT\n03:34 - Writing Output to file\n04:19 - Visualizing the task schedule\n04:51 - Comparing with GPT-4\n07:00 - Final Thoughts"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Given this Youtube video transcript: 0:00\nin this video you're going to learn how\n0:01\nto ask questions to a code base using\n0:03\nlink chain let's go so first things\n0:05\nfirst to set up this environment you're\n0:07\ngoing to need to install the um a few\n0:11\npackages that you know are just common\n0:13\nwhen you're using linking so in this\n0:15\ncase link change Deep Lake open Ai and\n0:18\ntick token\n0:19\nso then for our Imports we're going to\n0:22\nimport OS we're going to import get pass\n0:24\nwhich actually we're not going to be\n0:25\nusing because we're going to do the\n0:26\nauthentication with the apis just by\n0:29\nsetting the environment variables we're\n0:31\ngoing to then import open AI embeddings\n0:33\nto do our embeddings of the code base\n0:36\nthat we're going to be loading we're\n0:38\ngoing to do the vectorization using deep\n0:40\nlake which is the approach they used in\n0:42\nthe documentation\n0:43\nthen you're gonna set up your active\n0:46\nLoop token API key which you can do by\n0:50\njust going to the Blake website setting\n0:53\nup an account and acquiring your API key\n0:57\nhere you could also set up your open AI\n1:00\nAPI key I already have mine as an\n1:02\nenvironment variable in my system and\n1:05\nthen we're going to load the embeddings\n1:08\nfor using the open air embeddings class\n1:13\nthen what we're going to be doing is\n1:14\nwe're going to clone the repository that\n1:16\nwe're interested in in this case I'm\n1:18\ncloning a repository called motion\n1:20\ncanvas which is a typescript framework\n1:22\nto generate animations through code and\n1:25\nI'm really interested in this framework\n1:26\nbecause I'm learning how to code\n1:27\nanimations so I but there's like a lot\n1:31\nof things in this specific repo they're\n1:33\na bit confusing and a bit complicated so\n1:36\neven if you follow their actual tutorial\n1:39\nso what I'm doing here is I'm just gonna\n1:41\nsee fling Ching maybe can help me\n1:43\nthrough this question answering\n1:46\nsetup\n1:48\nokay so following\n1:50\num\n1:51\nmoving on we're going to be loading all\n1:53\nthe files inside the repo we're going to\n1:55\nbe using the text loader to do the\n1:58\nloading of the tax documents which will\n2:01\nbe the files in the code base\n2:03\nwe're going to set up the root directory\n2:05\nas the directory that contains the um\n2:08\ncode that we cloned\n2:11\nthen we're going to set up a empty list\n2:14\nfor storing the docs that we're gonna\n2:17\nload from the code base then we're going\n2:20\nto Loop through that code base and for\n2:22\neach of the files we're going to try to\n2:23\nload it with the text loader\n2:26\nand then\n2:28\num append that at that two hour uh docs\n2:32\nlist which will be a list with a bunch\n2:34\nof document objects that will be the\n2:36\nresult of loading each file with this\n2:37\ntext loader\n2:39\nthen if that doesn't work we're going to\n2:41\nset up an exception and we're gonna move\n2:43\non all right perfect so now after that\n2:47\nwhat we're going to be doing is here I'm\n2:49\njust\n2:50\num making sure that the docs list is\n2:53\nactually filled with documents so I'm\n2:55\njust printing the length of that list in\n2:58\nthis case I have 1 321 files that were\n3:00\nloaded and if I came here and I did\n3:03\nsomething like okay so print\n3:06\nlength of docs but then I showed you\n3:08\nguys what docs is it's just gonna be a\n3:11\nlist full of document objects that\n3:14\ncontains the files and the content of\n3:16\nthe files from the code base\n3:18\nokay so then we're gonna chunk those\n3:21\nfiles and we're going to do that using\n3:23\nthe character text Splitter from link\n3:25\nchain and that's as easy as just calling\n3:28\ncharacter text splitter we're gonna set\n3:30\nup some chunk size and some chunk\n3:33\noverlap it's important to note that for\n3:34\nthis example I didn't tune none of these\n3:37\nparameters but you can do that yourself\n3:39\nif you think that it's not working for\n3:41\nyour particular use case\n3:43\nthen we're going to split the documents\n3:46\nfrom the docs and save that to a\n3:49\nvariable called texts\n3:51\nso then here it was creating all the\n3:53\nchunks with different sizes from all\n3:56\nthose documents from the list\n3:57\nand then once we've done that now it's\n4:00\nthe vectorization bit so now we're going\n4:02\nto set up\n4:04\num we're going to create a public data\n4:07\nset although right now I'm not 100 sure\n4:09\nit's public just because I'm not setting\n4:11\npublic here because I think that they\n4:13\nremoved this parameter from a later\n4:16\nrelease of the Blake I don't know\n4:19\nhowever we're just going to be setting\n4:22\nup the Blake and then we're going to set\n4:24\nup the data set and the data set is\n4:25\ngoing to be based on this\n4:28\num the code base that we're trying to\n4:30\nthat were that we embedded or indexed\n4:32\nthen we're going to add all the\n4:34\ndocuments that we just chunked\n4:36\nhere\n4:39\nand\n4:40\nthat is pretty much it that's pretty\n4:43\nmuch we get what we got to do to do the\n4:46\nvectorization part so then it's going to\n4:48\nbe doing all the indexing\n4:50\nand once we are done\n4:53\nwe can then access it so now I'm\n4:56\naccessing that database\n4:58\nand\n5:00\nnow here I'm accessing the database and\n5:03\nonce we've accessed the database it's\n5:05\ngoing to print something like the data\n5:06\nthat can be visualized in Jupiter\n5:07\nnotebook Etc uh this data set was loaded\n5:11\nsuccessfully it is saved in whatever\n5:14\nPlace\n5:15\netc etc and then it's going to give some\n5:17\nmeta information about the data set like\n5:19\nthe size of the embedding and stuff like\n5:20\nthat\n5:21\nand now we can set up our retriever\n5:23\nwhich is going to be the thing that's\n5:25\ngoing to allow us to ask questions to\n5:27\nthe code base\n5:29\nyep so for the retriever we can set up\n5:33\nsome search keyword arguments like the\n5:36\ndistance metric which in this case if\n5:38\nI'm not mistaken this is cosine\n5:39\nsimilarity\n5:41\nand then the number of things that it\n5:43\nfetches if I'm not mistaking there's a\n5:45\nhundred uh and marks more marginal\n5:48\nrelevance to be 100 honest I'm not sure\n5:50\nwhat it is\n5:52\nuh but okay so this is I think this\n5:55\npretty much that this will return the\n5:58\ntop 10K answers and then from those it\n6:02\nwill give the the best one I think it's\n6:04\nsomething like that not a hundred\n6:06\npercent sure\n6:07\nokay but we set up these search keyword\n6:09\narguments for the retriever and then we\n6:13\ncan now just set up the chat with open\n6:15\nAI so that we can ask the questions and\n6:17\nthen just get the responses so we're\n6:20\ngonna load Channel open a chat open AI\n6:22\nfrom the linkchang.chat models and we're\n6:25\ngonna load the conversational retrieval\n6:28\nchain\n6:29\nfrom link Chang dot chains so we set up\n6:31\nour model in this case I'm loading\n6:33\ndirectly gpt4 you could load gp3 3.5\n6:37\nturbo like hbt it's up to you and you\n6:41\nsee what works best for your case\n6:43\nand then we're going to set up the\n6:45\nconversational retrieval chain and then\n6:47\nwe're gonna call the from llm method and\n6:51\nwe're going to feed that model that we\n6:53\njust loaded here and the retriever that\n6:55\nwe just set up in the previous cell\n6:58\nright here\n7:00\nso now with this we have all we need to\n7:03\nstart querying\n7:05\nto start querying the code base so I'm\n7:08\ngoing to set up a list\n7:10\nwith all the questions that I have for\n7:11\nthe code base in this case I just asked\n7:13\nchoose simple questions like how to\n7:14\ncreate an animation of a moving Square\n7:16\nmotion canvas and what are the core\n7:20\ncomponents to create an animation in\n7:22\nmotion canvas\n7:23\nuh and when I and once I've\n7:28\nset up these questions I can now set up\n7:31\nan empty list that will store the chat\n7:33\nhistory between me and the code base\n7:35\nthroughout this conversation\n7:37\nand then for each question in my\n7:40\nquestions list I'm gonna\n7:42\nquery the database I'm going to query\n7:45\nthat retriever chain\n7:46\nand then I'm gonna append the question\n7:51\nand the answer to my chat history\n7:54\nin print the out print the question and\n7:56\nprint the answer so it's as simple as\n7:58\nthat and now we can I can run this\n8:02\nand get the response\n8:06\noh sometimes it takes a little bit to\n8:08\nload I'm sorry to to run\n8:12\nso I'm going to do a magic trick\n8:14\nchildren or I guess so it took a while\n8:17\nbut here we have our answers so it went\n8:21\nas far as output the entire code that I\n8:24\nneed to do what I wanted to do in motion\n8:26\ngavis for my first question and then for\n8:29\nthe second question it's it discusses\n8:32\nthe main components of the motion motion\n8:35\ncamera so these are sets of elements\n8:37\nlet's play on the screen along with\n8:38\nanimations Etc by combining these\n8:40\ncomponents you can create complex and\n8:41\ninformative Vector animations all right\n8:44\ncool we can see that in a text editor\n8:46\nso this was the question what are the\n8:49\ncomponents to create an animation then\n8:51\nthe answer\n8:52\nto create you need the following group a\n8:54\nproject configuration project yeah\n8:56\nthat's all correct array of scenes\n8:59\nthat's correct scenes yeah that's\n9:02\nperfect\n9:03\nso it works it's amazing so I'm very\n9:06\nsatisfied with this answer all right\n9:08\nguys that's it that's how you query a\n9:10\ncode base using link chain if you like\n9:11\nthis video don't forget to like And\n9:13\nsubscribe and see you next time I want you to create a chapters section for this Youtube video with the following format:\n\ud83d\udcda Chapters:\n<double digit:time stamp> - <Concise phrase of a major part of the video>"}, {"role": "assistant", "content": "\ud83d\udcda Chapters\n00:00 - Introduction  \n00:01 - Setting up the environment  \n00:07 - Importing modules and API authentication  \n00:22 - Cloning and loading the repository  \n01:53 - Loading the files in the repository  \n02:14 - Chunking the files  \n03:00 - Vectorizing the documents  \n04:07 - Creating a dataset and adding documents  \n04:53 - Accessing the database  \n05:23 - Setting up the retriever  \n06:15 - Setting up a chat with OpenAI  \n07:05 - Querying the codebase  \n08:02 - Printing the answers  \n08:46 - Conclusion"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Given this Youtube video transcript: Introduction\n0:00\ntoday we're looking at this paper three\n0:01\nof thoughts deliberate problem solving\n0:03\nwith large language models now\n0:05\nessentially the authors developed this\n0:07\nframework that injects planning and\n0:09\ndecision making capabilities into large\n0:11\nlanguage models they do that by setting\n0:14\nup a system that leverages the\n0:16\ngeneration of what they call thought\n0:17\nsequences as well as self-evaluation in\n0:21\norder for the model to be able to\n0:23\nevaluate the quality of those thoughts\n0:25\nbased on how much they progress the\n0:27\nproblem forward it's really interesting\nFramework\n0:29\nbecause this framework allows the system\n0:31\nto reason over possible paths towards\n0:34\nsolving a problem and have have a way to\n0:38\npick one path that seems more promising\n0:41\nthan the other as well as look ahead or\n0:43\nbacktrack to make better decisions and\n0:45\nto adapt in different situations now\n0:47\nthey start by comparing similar\n0:49\nprompting approaches like input output\n0:51\nprompt where you're just directly\n0:53\nprompting the model to solve the problem\n0:55\ngiving a few Test instructions or some\n0:58\nfew shot examples and then they talk\n1:00\nabout Chain of Thought prompting where\n1:02\nthe key idea is to introduce a chain of\n1:04\nthoughts between the input X and the\n1:06\noutput Y where these thoughts are\n1:08\nlanguage sequences that somehow move the\n1:10\nproblem forward finally they talk about\n1:12\nChain of Thought with self-consistency\n1:14\nwhere essentially you're sampling from a\n1:17\nchain of thoughts and then picking the\n1:18\noutput that's the most frequent now the\n1:20\nauthors argue that there are two\n1:21\nproblems with existing approaches first\n1:23\nthere's a lack of local exploration of\n1:26\nthought of thought sequences secondly\n1:28\nthey argue that current approaches lack\n1:30\nthe incorporation of decision making and\n1:32\nplanning abilities nor do they have the\n1:34\nability to backtrack or look ahead to\n1:36\nhelp them evaluate different options now\n1:38\nwith this dot framework the author\n1:40\ndivides the problem into four stages you\n1:43\nhave the decomposition stage the\n1:45\ngeneration stage the evaluation stage\n1:47\nand the search stage now in the\n1:50\ndecomposition stage the properties of\n1:52\nthe problem are used to design and\n1:54\ndecompose thought steps where each\n1:56\nthought should be small enough so that\n1:58\nllms can generate a diverse range of\n2:02\npossible thought sequences from that\n2:04\noriginal thought as well as big enough\n2:06\nso the large language models can\n2:08\nevaluate them meaningfully in terms of\n2:10\nhow much they progress the problem\nGeneration\n2:12\nforward in the generation stage you're\n2:14\nsampling from a distribution of possible\n2:16\nthoughts which works well When the\n2:19\nThought space is Rich or you use a\n2:21\nproposed prompt to propose a set of\n2:22\npossible thoughts from that state then\n2:25\nyou go into your evaluation stage where\n2:27\nyou're going to evaluate the quality of\n2:29\nthose thoughts in terms of how much they\n2:31\nget you closer to solving the problem\n2:33\nnow in the evaluation State you can\n2:34\neither value each state independently or\n2:37\nyou can use votes where the model is\n2:39\ngoing to be comparing States and picking\n2:42\nthe one that seems more likely to\n2:44\nsucceed and get you closer to a solution\nEvaluation\n2:46\nand then for the search stage the\n2:48\nauthors argue that you can just plug in\n2:50\nplay different search to research\n2:52\nalgorithms and they chose the simplest\n2:54\nones so they chose a breakfast search\n2:56\nand that first search now for the\n2:59\nresults they did they use three data\n3:01\nsets sort of because they scrape much of\n3:04\nthe data to the checking for like\n3:05\nimprovised benchmarks or something like\n3:07\nthat so the first one is game 24 where\n3:10\nyou you're given four numbers and you\n3:12\ncan use mathematical operations to reach\n3:15\nthe number 24 and then they created a\n3:17\ncreative writing task where essentially\n3:19\nthe models are given four random\n3:21\nsentences and they have to form passage\n3:23\nof four paragraphs ending in those four\n3:25\nsentences and finally they also tested\n3:27\nin a set of crossword tasks here for\n3:29\nfigure two they're showing how the\n3:31\nproposed prompt is proposing a set of\n3:33\nthought steps from that initial input in\n3:37\nthis case 4 9 10 and 13. so then the\n3:39\nthought generation is just generating\n3:41\npossible options of what you can do with\n3:43\nthese numbers given the problem in the\n3:45\nfirst place in this case the problem of\n3:47\narranging a set of mathematical\n3:48\noperations around those numbers to\n3:50\nobtain the number 24 and then in the\n3:52\nvalue prompt the model is evaluating how\n3:54\nmuch each of those possible thoughts is\n3:57\ngetting us closer to obtaining the goal\n4:00\nwhich is the number 24 and then as for\nResults\n4:02\nthe results they show that the tree of\n4:03\nthought approach had a 74 success in\n4:06\nthis case in this task in comparison to\n4:08\nthe other methods there I think there\n4:09\nare some things that we could consider\n4:11\nabout like how they use the other\n4:13\napproaches because they did some very\n4:15\nspecific type of prompts for the i o\n4:18\napproach the Chain of Thought and\n4:19\nself-consistency but I thought 74 in\n4:22\nthis game of 24 was a really cool result\n4:23\nnow for the creative writing desk they\n4:26\nhad this input where the would be like\n4:29\nto prompt the model to Riker in four\n4:31\nparagraphs ending those four sentences\n4:33\nand then the model would generate plans\n4:35\nof writing options and then the LM\n4:38\nitself would vote over those options to\n4:41\ndecide which plan was best and then they\n4:43\nshowed that the three or thought\n4:44\napproach had a higher coherence score\n4:45\nnow for the crossword puzzle they argue\n4:48\nthat they chose this task not not\n4:49\nbecause they wanted something that could\n4:51\nsolve it better than anything else\n4:52\nbecause they they said that there are\n4:54\nother natural language approaches a\n4:56\nnatural language processing approaches\n4:57\nthat could do this better but they\n4:58\nwanted to show that the more although\n5:00\nhad General capabilities in terms of\n5:02\nproblem that involves looking ahead\n5:04\nmaking plans deciding having to go back\n5:07\nEtc because the crossword puzzle is a\n5:09\nproblem that involves somewhat\n5:10\ndeliberate reasoning and looking ahead\n5:13\nbacktracking adapting and then they show\n5:16\nthat you know the the true thought\n5:18\napproach would propose thoughts based on\n5:20\nthe input to get the crossword puzzle\n5:22\nright and then use something like a\n5:24\nsearch algorithm like DFS in order to\n5:27\nsearch the space of possible options and\n5:29\nthen have a state evaluator classify\n5:32\neach of those options with something\n5:34\nlike sure impossible maybe in terms of\n5:37\nhow much it would progress the problem\n5:39\nforward and solve the crossword puzzle\n5:40\nthey claim that the tot approach\n5:43\nimproved significantly all the metrics\n5:46\nwith comparison to the other approaches\n5:48\nand they had a success rate of 60\n5:50\nfinally arguing that such an improvement\n5:53\nis not surprising given that I O and cot\n5:55\nlack mechanisms to try different Clues\n5:57\nand make changes to decisions or\n5:59\nbacktrack track which is something that\n6:00\nthis approach has now they finish up by\n6:03\ntalking about related work where they\n6:04\nargue that the three of thought\n6:05\nframework extends existing Planning\n6:07\nformulations by considering multiple\n6:09\npotentially feasible plans\n6:11\nsimultaneously at each problem-solving\n6:13\nstep and proceeding with the most\n6:14\npromising ones then they finish up by\nRelated Work\n6:17\ntalking about how this approach is\n6:19\nmodular and flexible adaptable and\n6:22\ndespite the fact that you need more\n6:24\nresources like you know the cost of the\n6:26\ngpt4 API to run an approach like this uh\n6:30\nthey argue that their modularity allows\n6:33\nyou to customize the performance and\n6:35\ncost trade-off finally they also argue\n6:37\nthat tot improves interpretability which\n6:39\nis a big topic right now because of\n6:40\nalignment and they finish up talking\n6:43\nabout this idea of system one and system\n6:44\n2 where the contest of the three or\n6:46\nthought framework the associative system\n6:48\none can be beneficially augmented by\n6:50\nSystem 2 based on searching a tree of\n6:52\npossible paths to the solution of a\n6:53\nproblem so you had system one generate a\n6:56\nset of possible options and then system\n6:58\ntwo B this kind of search over that\n7:01\nspace to find one that better reaches\n7:05\nthe solution but yeah overall I really\n7:06\nlike this paper I think this approach is\n7:08\nquite interesting I think the the\n7:10\narguing for problem solving like framing\n7:15\nproblem solving as three search over a\n7:18\ncombinatorial space of possible states\n7:21\nthat will get you to the solution it's\n7:22\nnot exactly a new approach which is\n7:24\nsomething that the authors comment on\n7:26\nthe beginning that they inherited this\n7:28\napproach from Newell and that's\n7:31\nsomething that's not new I think one of\n7:33\nthe main key contributions of this paper\n7:35\nis this ability to self-evaluate through\n7:38\nthis combination of proposing possible\n7:40\noptions and then heuristically searching\n7:43\nthrough the best ones by evaluating\n7:46\nwhich ones get you closer to the\n7:47\nsolution and the way they do the\n7:49\nevaluation by using this by using a\n7:52\nmechanism where the model is reasoning\n7:56\nover potential States you're leveraging\n7:59\nthe yellow capabilities to not only\n8:02\ngenerate options to solve the problem\n8:04\nbut to evaluate their quality and then\n8:07\nmove the problem forward and I think\n8:09\nthis approach is really cool so thanks\n8:11\nfor watching don't forget to like And\n8:12\nsubscribe and see you next time cheers I want you to create a chapters section for this Youtube video with the following format:\n\ud83d\udcda Chapters:\n<double digit:time stamp> - <Concise phrase of a major part of the video>"}, {"role": "assistant", "content": "\ud83d\udcda Chapters\n00:00 - Introduction\n00:01 - Overview of the Research Paper\n00:14 - The Idea of Thought Sequences\n00:29 - System's Reasoning and Decision-Making Capabilities\n00:49 - Comparing Different Prompting Approaches\n01:02 - Chain of Thought Prompting\n01:14 - Chain of Thought with Self-Consistency\n01:21 - Problems with Current Approaches\n01:43 - Decomposition Stage\n01:50 - Generation Stage\n02:10 - Evaluation Stage\n02:34 - Search Stage\n02:59 - Research Results\n03:10 - Game 24 Task\n03:19 - Creative Writing Task\n03:27 - Crossword Tasks\n04:00 - Evaluating the Tree of Thought Approach\n04:26 - Creative Writing Task Results\n04:48 - Crossword Puzzle Task Discussion\n05:39 - Success Rate of Tree of Thought Approach\n06:03 - Related Works\n06:19 - Modularity and Flexibility of the Approach\n06:37 - Improved Interpretability\n06:43 - System One and System Two Discussion\n07:06 - Conclusion"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Given this Youtube video transcript: 0:31\nfind the best prompt to create optimal\n0:34\nInky flashcards use a combination of\n0:36\nchargeptee Lan ching and some simple\n0:39\nprompt engineering experiments to\n0:41\nautomate the process of creating Anki\n0:43\nflash cards now a while back I used\n0:45\nchurch PT to automate this process of\n0:48\ncreating flashcards with a simple crop\n0:50\nlike create Inky flashcards from this\n0:52\npiece of text now the problem with this\n0:54\napproach is that you might run into\n0:56\nproblems like a reliable questions and\n0:58\nanswers the Fletcher might not reflect\n1:01\nall the information present in the text\n1:02\nand things like repeated questions which\n1:05\nis something I heard from this person\n1:07\nthat commented on the channel to bypass\n1:09\nthese issues what I decided to do is use\n1:11\na concept from prompt engineering called\n1:13\ndecomposition where essentially to\n1:15\nfacilitate the process for the large\n1:17\nlanguage model you break down the\n1:19\nproblem into subtasks in this case I\n1:22\nbroke it down into two simple tasks\n1:24\nfirst I would like to extract all the\n1:26\nrelevant information from a giveaway\n1:28\npiece of text and then based on those\n1:30\nextracted facts create the questions and\n1:33\nanswers that I could then import to enki\n1:35\nI started by extracting all the facts\n1:38\nfrom a piece of text using gpt4 the most\n1:41\nadvanced large language model to date\n1:43\nfrom open AI which served to me as a\n1:45\nreference for a correct answer but my\n1:47\nultimate goal was to actually use chat\n1:50\nGPT for this automation since it's a\n1:52\nmuch faster model and much cheaper to\n1:54\nuse when we're using the actual API to\n1:57\ndo that I needed to make sure that the\n1:59\nanswers obtained with T would be\n2:01\nreliable so I also prompted gbt4 to\n2:04\ncreate a set of prompt candidates that I\n2:07\nwould then test using the actual\n2:09\nchargept API so I created a set of\n2:11\nprompt candidates and also added a few\n2:13\nmore with variations and I ended up with\n2:15\na set of 25 prompt candidates so then I\n2:19\nused Lane chain to create a set of\n2:21\nprompt templates that I would use to\n2:23\nprogrammatically query the charger TBI\n2:27\non a given text so after doing that I\n2:30\nhad the following problem to solve how\n2:31\ncan I evaluate the quality of the\n2:33\noutputs from church PT without manually\n2:36\nchecking each fact individually and\n2:38\ncomparing it to the original article so\n2:40\nmy solution to this was to use gpd4 as a\n2:43\nkind of a judge that would give a score\n2:45\nto each of the outputs of the thing\n2:47\nwhich hbt in order to rank them\n2:50\naccording to how well they extracted\n2:53\neffects from the text now based on these\n2:55\nscores I first generated a part chart to\n2:57\nshow all these scores from all the\n2:59\ndifferent pumps that I used and then I\n3:01\njust selected the best prompt given the\n3:04\nscores obtained using gpt4 now with this\n3:06\nbest prompt in mind my second task was\n3:09\nto actually create the Yankee flashcards\n3:10\nthe thing here is that creating the\n3:13\nactual questions and answers based on\n3:14\nfacts is a much simpler task for chech\n3:17\nPT than actually extracting the facts so\n3:19\nI came up with this prompt that actually\n3:21\nworked quite well and I created\n3:23\nsomething called a sequential chain\n3:25\nusing linking in order to concatenate\n3:28\nboth of these prompts into one automated\n3:31\nprocess that I could just run like I'm\n3:33\nshowing here after running I was quite\n3:35\nsatisfied with the results and I\n3:36\nconsider this task finished now I really\n3:38\nlike this approach where we can\n3:40\nintegrate space repetition systems with\n3:42\nlarge language models and I think\n3:44\nthere's a lot of potential to explore in\n3:46\nthese kinds of augmented Integrations\n3:48\nnow I wasn't 100 sure regarding how I\n3:51\nwas using gpt4 to evaluate the metrics\n3:54\nbecause that could view problems but\n3:56\nsince I didn't know exactly which was\n3:58\nthe best approach to take I decided to\n4:00\njust go with this one and see what\n4:02\nhappens for the future I intend to\n4:04\nexplore more these use cases of large\n4:06\nlanguage models as tools to automate\n4:09\nlower level processes if you like this\n4:11\nvideo don't forget to like And subscribe\n4:13\nand see you next time cheers I want you to create a chapters section for this Youtube video with the following format:\n\ud83d\udcda Chapters:\n<double digit:time stamp> - <Concise phrase of a major part of the video>"}, {"role": "assistant", "content": "\ud83d\udcdaChapters\n\n0:02 - Introduction to Flashcards \ud83c\udfb4\n0:06 - The Traditional Method of Creating Flashcards \ud83d\udcdd\n0:27 - Introduction to Prompt Engineering and Experiments \ud83d\ude80\n0:43 - The Process of Automating Flashcards with GPT \ud83e\udd16\n0:56 - Problem with the Traditional Approach \ud83d\udea7\n1:09 - How to Bypass these Issues using Decomposition \ud83d\udee0\ufe0f\n1:26 - Extracting Information using GPT-4 \ud83e\udde0\n1:50 - Why We Opted for ChatGPT in our Automation Process \ud83d\udcbc\n2:04 - Creation of Prompt Candidates for Testing \ud83e\uddea\n2:21 - Usage of Lan-chain to Create Prompt Templates \ud83d\udca1\n2:31 - Evaluating the Quality of Outputs without Manual Checking \ud83d\udcc8\n2:55 - Visualization of Scores using a Bar Chart \ud83d\udcca\n3:06 - Creating Anki Flashcards with the Best Prompt \ud83c\udfaf\n3:17 - Creating Questions and Answers with ChatGPT \ud83d\udcdd\n3:33 - Displaying the Final Automated Process \ud83d\udcab\n3:38 - Integrating Space Repetition Systems with Large Language Models \ud83d\udd04\n3:51 - Discussion on Metrics Evaluation and Potential Issues \ud83d\udd0d\n4:00 - Conclusion and Future Directions \ud83c\udf08"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Given this Youtube video transcript: Introduction\n0:00\nSo we all know that large language models make silly mistakes from big multiplications\n0:04\nto hallucinating information.\n0:05\nNow, what the authors of this paper, Faith and Faith, Limits of Transformers in Compositionality\n0:10\nasked is whether or not transformer based models are actually uncovering rules for problem\n0:16\nsolving and exploring true reasoning paths when they're solving problems or are they\n0:21\njust doing some fancy pattern matching between input and output features.\nQuestioning transformer-based models\n0:26\nThe authors argue that these silly trivial mistakes might indicate a lack of ability to\n0:31\nperform compositional reasoning in transformer based models, arguing that actually what\n0:36\ntransformers are doing are just reducing multi step compositional reasoning into some linearized\nLack of ability to perform compositional reasoning\n0:43\ngraph matching to evaluate the compositional reasoning abilities of transformers.\n0:47\nThey set on three tasks to assess these capabilities.\n0:50\nA long form multiplication task, the Einstein puzzle and a dynamic programming problem.\n0:55\nNow these tasks were chosen based on the fact that they're centered around compositionality\n0:59\nand that they require the combination of reasoning operations with following a computational\nThree tasks to assess capabilities\n1:04\npath to arrive at correct solution.\n1:06\nThey form two hypotheses for this paper.\n1:08\nThe first one is that transformers are actually reducing multi step compositional reasoning\n1:12\nto linearized graph matching, which is something that is taught to contrast with multi step\n1:16\nreasoning that learns to apply computational rules to reach correct answers.\n1:21\nThe second hypothesis error propagation, which is this idea that because transformers may\n1:24\nhave inherent limitations in solving high complexity compositional tasks that exhibit\n1:28\nnovel patterns, errors in the early stages may lead to substantial compounding errors\nHypotheses for the paper\n1:33\nin these subsequent steps, preventing the models from reaching correct solutions.\n1:37\nTo investigate these hypotheses, they formulated compositional tasks as computation graphs,\n1:41\nbreaking down problem solving to sub modular functional steps, enabling structured measurements\n1:46\nof complexity and the verbalization of computational steps as input sequences to language models.\n1:52\nNow here in figure one, they show an example of a computation graph for a long form multiplication\n1:56\nalgorithm A with inputs seven and 49.\n1:59\nThey used graph metrics to quantify complexity.\n2:01\nThey used reasoning depth as a proxy for the maximum level of multi hop reasoning required\n2:06\nto solve the test, reasoning with as the maximum number of variables required to keep in parallel\n2:13\nduring the computation and average parallelism, which is the average with new computation\nFormulating compositional tasks as computation graphs\n2:17\nthrough the graph.\n2:18\nThey used this approach of predicting surface patterns to relative information gain to assess\n2:22\nthe strategies for partially correct answers models used in incorrect responses.\n2:27\nYou had three tasks for this paper.\n2:29\nThe first one was multi-digit multiplication, where you have to manipulate symbols through\n2:33\nthe usage of procedural rules.\n2:35\nYou had the Einstein puzzle, which involves a list of houses and a list of attributes,\n2:39\nand you have to associate the attributes with the houses to the usage of clues and constraints.\n2:45\nAnd you had a dynamic programming problem that reads as given a sequence of integers,\n2:50\nfind a subsequence with the highest sum such the no true numbers of the subsequence are\n2:53\nadjacent in the original sequence.\n2:55\nNow here in figure two, they show that a zero shot accuracy of the models decreased as the\n2:59\nproblem sized increase in each of the tasks.\nThree tasks explained\n3:02\nAnd they also showed that the average parallelism, which is one of the metrics they used to quantify\n3:06\ncomplexity was negatively correlated with accuracy of the models.\n3:09\nNow here in figure three, they showed that the GP3 fine tune exhaustively on task specific\nZero shot accuracy and complexity metrics\n3:13\ndata only did well in examples present in the distribution.\n3:17\nNow one of the things the authors wanted to understand is that why is it that transformers\n3:20\ncan predict partially correct answers when the overall response is incorrect.\n3:24\nNow they hypothesized that that was the case because of peculiarities in the task distribution\n3:29\nthat allowed the model to guess partial answers without performing the multi-step reasoning\n3:34\nrequired to solve the task.\n3:36\nHere in figure four, they show examples of poor generalization for the GP3 in the puzzle\n3:41\nand DP tasks.\n3:43\nHere in figure five and six, the authors are showing how the ratio of correct nodes, meaning\nPoor generalization and partial solutions\n3:48\ncorrect partial solutions provided by the models, decreases as the problem size increases\n3:54\nin terms of the number of graph layers, which is one metric they used to quantify complexity.\n3:59\nFinally, the authors present a mathematical argument showcasing why is the case that as\nRatio of correct nodes and decreasing correct answers\n4:03\nthe problem size increases, the chance of the models to produce a correct answer decreases\n4:08\nexponentially.\n4:09\nThe authors conclude that transformers often solve multi-step compositional problems by collapsing\nMathematical argument for reducing correct answers\n4:13\nthe depth of compositional operations via analogical pattern matching.\n4:17\nNow there are a lot of things that I liked about this paper, framing problem solving\n4:20\nas computation graphs, quantifying complexity to these graph metrics and showing strong\n4:25\nevidence of the limitations of transformers for compositional reasoning.\n4:28\nIf you like this video, don't forget to like and subscribe and see you next time.\n4:32\nCheers. I want you to create a chapters section for this Youtube video with the following format:\n\ud83d\udcda Chapters:\n<double digit:time stamp> - <Concise phrase of a major part of the video>"}, {"role": "assistant", "content": "\ud83d\udcda Chapters:\n00:00 - Introduction\n00:26 - Questioning transformer-based models\n00:43 - Lack of ability to perform compositional reasoning\n01:00 - Three tasks to assess capabilities\n01:33 - Hypotheses for the paper\n02:17 - Formulating compositional tasks as computation graphs\n03:00 - Three tasks explained\n03:13 - Zero shot accuracy and complexity metrics\n03:48 - Poor generalization and partial solutions\n04:03 - Ratio of correct nodes and decreasing correct answers\n04:13 - Mathematical argument for reducing correct answers\n04:39 - Conclusion and final thoughts"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Given this Youtube video transcript: Introduction and Update\n0:00\nwhat's up guys so in this video I want\n0:02\nto do a quick update on some stuff that\n0:04\nI've been doing over the past two weeks\n0:05\nso I took a break from YouTube for a\n0:07\ncouple weeks to work on some personal\n0:09\nprojects and I want to share some of\n0:11\nthese projects with you specifically\n0:13\nI've done I did the like training course\nO'Reilly Training Course Experience\n0:16\nfor O'Reilly my first course doing my\n0:19\nfirst live training course for them and\n0:21\nit went amazing the light training was\n0:24\nabout building text-based applications\n0:26\nwith the judge TPI and link chain and I\n0:29\nmean we had over 275 attendees live and\n0:34\nI mean I got hundreds of questions great\n0:36\nfeedback uh so it was just an amazing\n0:39\nexperience and we're gonna do another\n0:41\none in October on the 20th of October\n0:45\nand I'm Gonna Leave a link in the\n0:46\ndescription if you want to join that you\nResearch on Building LLM Agents\n0:48\nget and besides that I've been getting\n0:50\nin building LOL agents I've been\n0:53\nresearching a lot about how to build llm\n0:55\nagents but not only that researching a\n0:57\nlot about the framers researching a lot\n1:00\nabout how to formalize building agents\n1:04\nand understanding what the problem is\n1:07\nwhen we're kind of like attaching large\n1:10\nlanguage models to tools and asking the\n1:15\nquestion okay how can I make this thing\n1:18\ndo useful things reliably right this is\n1:21\na really tough\n1:22\nit's a really tough thing to do and\n1:27\num recently during my vacation I read\nOverview of Autonomous AI Agents Paper\n1:29\nthis 32 Pages paper on a survey for\n1:32\nautonomous AI agents again I'll leave a\n1:34\nlink in the description essentially this\n1:37\npaper was really cool because it kind of\n1:39\ngave me like a perfect overview of\n1:42\neverything that's going on in research\n1:44\nregarding building llm-based agents and\n1:47\nI want to share a few of these insights\n1:50\nwith you guys because essentially it's a\n1:51\nsurvey that just covers a bunch of stuff\n1:54\nfrom you know how to think about these\n1:56\nagents what research is happening right\n1:59\nnow or all these types of problems types\n2:02\nof applications of agents and stuff that\n2:04\npeople have been doing for example they\n2:06\ngive one example inside the paper of a\nExamples of LLM-based Agents in Research\n2:10\nchemistry researcher agent that\n2:12\nessentially was able to synthesize a new\n2:14\nmolecule just using you know LMS plus\n2:18\nchemistry specific tools and I thought\n2:21\nthat was one of the coolest Keepers I\n2:23\nthink it's called Kim Crow I'll put a\n2:25\nlink in the description for both the\n2:28\nsurvey paper and this chemistry agent\n2:30\npaper I thought it was amazing right in\n2:32\nthis paper they talk about how you can\n2:34\nthink about are the lens in terms of\n2:35\nthese four basic components like a\n2:38\nprofile so that's essentially how do you\n2:40\nkind of label what the agent is it can\n2:43\nbe a researcher like a content creator\n2:45\nwhatever you want\n2:47\num and then they talk about okay so then\n2:49\nyou can manually describe what the agent\n2:52\nis give it a profile like you're a\n2:54\nresearcher your mathematician Etc or you\n2:57\ncan have vanilla live the fine with the\n2:59\nagent is or you can do something called\nComponents of LLM Agents: Memory and Planning\n3:01\ndata set alignment so the second\n3:03\ncomponent it's is memory and you know\n3:05\nwhen you're interacting with llm agents\n3:08\nyou needed to have some sort of memory\n3:10\nsome sort of ability to hold information\n3:12\nfrom past experiences and they talk\n3:16\nabout what are the challenges there you\n3:18\nknow they talk about something like\n3:19\nunify and hybrid memory and the types of\n3:22\nformats of memory and the types of\n3:24\noperations that you have to do and they\n3:27\ngo into detail about that so that was\n3:28\ncool so then they go into planning so\n3:32\nthese agents need to plan out their\n3:34\nactions before right remember that most\n3:36\nof the like really high performing\n3:38\nagents to date are essentially based off\n3:40\nthis paper called react which is I'm\n3:43\ngoing to leave a link in the description\n3:44\nfor this paper as well and it's a\n3:46\nessentially it's a paper about you know\n3:48\nhow an llm uh how to like get the best\n3:52\nout of vanilla them by just having it\n3:56\num interpolated like just put\n3:58\nplanning in actions kind of like one\n4:01\nafter each other so that it gets the\n4:04\nbest performance by just having it\n4:06\nhaving it like okay so this is what I'm\n4:07\ngonna do and then it does and then it\n4:09\nrethinks and then it goes and does again\n4:11\nI mean this is a really cool fundamental\n4:14\npaper so planning is a key component of\n4:16\nthese agents and uh they they talk a lot\n4:20\nabout strategies in planning and types\n4:22\nof feedback that you can get during the\n4:24\nplanning phase like you can have just\n4:26\nthe model plan out its own tasks and\n4:28\nthen execute them or without any\n4:31\nfeedback or you can have feedback for\n4:33\nthe planning and the quality of the\n4:34\nplanning that the model will Define for\n4:36\nitself\n4:37\nand finally you have the action space\nAction Space and Strategies in Planning\n4:40\nwhich is actually they go into like\n4:43\nbrilliant detail like this hierarchical\n4:45\nstructure almost of a lot of stuff\n4:48\nrelated to how agents take actions in\n4:52\nthe real world and essentially you know\n4:54\nyou can have actions based off the tools\n4:56\nthat the agents have right like\n4:57\nsearching the internet uh searching\n5:00\nGoogle running code Etc but you can also\n5:03\nhave actions based off the\n5:05\nself-knowledge of the agent\n5:06\nuh they go into a bunch of details about\n5:09\nlike different things to think about for\n5:11\nexample the action influence something\n5:14\nthey call Action influence which is like\n5:16\nto analyze how a nation of an agent uh\n5:19\nhas certain consequences certain you\n5:21\nknow from when I do this what happens\n5:24\nwhat are the consequences to my action\n5:26\nfor example if it does something on the\n5:28\nInternet what happens from there\n5:31\nstuff in that sense so that was kind of\n5:33\ninteresting and I mean this paper is\n5:36\nfull of cool stuff like for example\n5:37\nhaving tables for the benchmarks for\nChallenges in Benchmarking LLM Agents\n5:40\nthese agents because essentially\n5:41\nbenchmarking llm agents today is like a\n5:44\nbig problem because you know in essence\n5:47\nyou have\n5:49\num you don't have any kind of structure\n5:51\nright now any kind of framework or\n5:52\nprotocol or how to evaluate the quality\n5:55\nof these agents and it's also very hard\n5:57\nbecause the tasks that these agents are\n5:58\nsupposed to do are very general so it's\n6:00\nvery subjective in a specific sense\n6:03\nhowever they do mention papers like\n6:05\nagent bench which is a paper that just\n6:08\ntry to defy this protocol and framework\n6:11\nfor how to properly evaluate llm agents\n6:14\nand it's kind of nice because they have\n6:15\na bunch of tasks and they're within\n6:17\nthose tasks you can see you know and you\n6:20\nhave many different ways of evaluating\n6:22\nuh Carl Black's things like reasoning\n6:25\nand problem solving capabilities\n6:27\nso this paper is quite interesting and\n6:29\nit got me thinking about doing a few\nUpcoming Projects and Researcher Agents\n6:31\nprojects with these agents I already\n6:33\nstarted some and you guys are probably\n6:35\ngoing to see that in the very near\n6:36\nfuture essentially I want to build my\n6:39\nown researcher agents and you know the\n6:41\ndifferent types of Agents related to\n6:43\ndifferent things\n6:44\nand I think that would be a really cool\n6:47\nproject to have in the channel\n6:49\nand the last thing was I've been getting\n6:51\ninterested in La Machu which is the\nInterest in Lamma 2 and Open Source Models\n6:54\nrecently released open source model by\n6:57\nmeta where essentially you have a model\n7:00\nthat you know Rivals Church B team\n7:02\nperformance but it's completely open\n7:04\nsource and you can download it to your\n7:07\ncomputer and essentially you have a\n7:09\nlarge language model that can do\n7:10\nwhatever set up a server in my new\n7:12\nworkspace which is actually a great\n7:15\nsegue to my next topic of discussion\n7:18\nwhich is I've been over the last month\nNew Workspace and Design Plans\n7:22\nand a half designing my new workspace I\n7:26\nkind of like I'm gonna assure you guys a\n7:28\nphoto of what the space looks like right\n7:30\nnow but it's actually just I have a\n7:31\nsmall office where I do the videos and\n7:34\nwhere I work and I want to change the\n7:36\nspace to make it like not only better\n7:39\nfor making YouTube videos but better to\n7:41\nwork better to build stuff I've been\n7:43\ngetting into electronics and you know\n7:45\ndoing some kind of like you know maker\n7:48\ntype projects so when I have a little\n7:49\nworkshop inside my workspace\n7:51\nand this project has been really\n7:54\ninteresting because it got me kind of\n7:56\nsketching a lot I've been trying to\n7:57\nlearn how to draw for a while now and\n8:00\nI've been drawing kind of like I drew\n8:02\nout some designs for like the floor plan\n8:04\nof the workspace and it was really cool\n8:07\nbecause I've learned a lot about how\n8:10\npeople in architecture designed their uh\n8:14\nyou know do sketches for different\n8:15\narchitectural designs and from that I\n8:20\nkind of like evolved to like more you\n8:22\nknow slightly more formal floor Blends I\n8:26\nmean not really not really formal\n8:28\nbecause they they don't have like\n8:30\nspecific measurements and stuff like\n8:32\nthat but they're a little bit more like\n8:34\nstructured and from that what I did was\n8:37\nI used a tool called relative Fusion\n8:39\nwhich I'll leave a link in the\n8:41\ndescription they're not sponsoring this\n8:42\nvideo by the way\n8:43\nand I really liked the video I really\nUsing AI for Rendering Floor Plans\n8:45\nlike the tool because essentially it\n8:47\nallowed me to render\n8:49\nmy sketches so I have these floor plans\n8:53\nand I rendered them with AI using they\n8:56\nuse control net they had these they have\n8:59\nthis really cool radio\n9:01\n2 if you guys don't know what radio is\n9:03\nit's a framework for like quickly\n9:05\nbuilding a machine learning apps and\n9:07\nit's kind of like streamlit but it's\n9:09\nlike you know uh simpler and um you know\n9:12\nruns in the browser just like\n9:13\nstreamlined it's pretty cool\n9:16\num and it was quite I had a great\n9:19\nexperience doing these uh renderings of\n9:21\nthe sketches because they gave me kind\n9:23\nof like a visual\n9:24\neven not 3D but kind of 3D visual of\n9:28\nwhat the workspace would look like\n9:29\naccording to different settings for my\n9:32\nfloor plan designs\n9:34\nand I think that was really cool I would\n9:37\nlike to do a video on like how to use AI\n9:40\nlike how to use open source AI\n9:43\nto do projects like that I think that's\n9:45\na really cool kind of Avenue that we're\n9:48\ngoing with these things now in mid\n9:50\nJourney For example now you can do\nFuture of AI in Architecture and Design\n9:51\nsitting in the journey which is amazing\n9:53\nbecause you can you know generate\n9:56\nan image of like let's say uh like a an\n9:59\noffice or a couch or whatever and then\n10:02\nyou can select a small part within that\n10:04\nimage of an office or a couch or\n10:06\nwhatever and you can ask the model to\n10:09\niterate on variations just for that\n10:12\nspecific region that you selected\n10:14\nso there are a lot of tools for you know\n10:17\narchitecture and design that I think are\n10:19\ninteresting and I would love to see how\n10:22\nwe can have you know free open source\n10:25\nyou know available tools for the public\n10:27\nfor you guys to interact with and use it\n10:29\nin your own workflows I mean that's just\n10:32\nmy experience and I thought it was\n10:33\nreally cool like I like this idea of\nChannel Goals and Future Content\n10:36\nlooking at how people learn and how\n10:38\nmachines learn and just try to find the\n10:41\nintersections when we can build tools\n10:44\nthat you know improve and augment Our\n10:46\nLives I know it sounds kind of like\n10:48\ncliche and stuff but this is really the\n10:50\ngoal of the Shadow and I want to keep\n10:53\ndoing videos where I you know maybe in\n10:55\nthe future I won't even do podcasts I'll\n10:57\ntalk to people because my goal is to\n11:00\nhave a place where you can go and it has\n11:02\nlike the latest tools that you can use\n11:04\nto build something that helps you do\n11:07\nresearch it helps you do learning\n11:10\nand that's pretty much it this pretty\n11:12\nmuch covers what I've been doing lately\n11:14\nuh I'm working on some proposals for\nUpcoming Training Courses and Conclusion\n11:18\nsome new like training courses that I\n11:20\nwant to do in the future so I'll keep\n11:22\nyou guys updated all that all right\n11:24\nthank you guys for watching don't forget\n11:26\nto like And subscribe and see you next\n11:28\ntime cheers I want you to create a chapters section for this Youtube video with the following format:\n\ud83d\udcda Chapters:\n<double digit:time stamp> - <Concise phrase of a major part of the video>"}, {"role": "assistant", "content": "\ud83d\udcda Chapters:\n00:00 - Introduction and Update\n00:16 - O'Reilly Training Course Experience\n00:48 - Research on Building LLM Agents\n01:29 - Overview of Autonomous AI Agents Paper\n02:10 - Examples of LLM-based Agents in Research\n03:01 - Components of LLM Agents: Memory and Planning\n04:40 - Action Space and Strategies in Planning\n05:40 - Challenges in Benchmarking LLM Agents\n06:31 - Upcoming Projects and Researcher Agents\n06:54 - Interest in Lamma 2 and Open Source Models\n07:22 - New Workspace and Design Plans\n08:45 - Using AI for Rendering Floor Plans\n09:51 - Future of AI in Architecture and Design\n10:36 - Channel Goals and Future Content\n11:18 - Upcoming Training Courses and Conclusion"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Given this Youtube video transcript: Intro\n0:00\nall right so I live in Lisbon so what I\n0:03\ndecided to do was I live in Lisbon which\n0:06\nis not exactly a huge town\nPope is coming to Lisbon!\n0:10\nokay so I live in Lisbon which is not\n0:12\nexactly a super big city it has\n0:14\napproximately three million inhabitants\n0:16\nand the pope is coming to visit on this\n0:18\nfrom the 2nd of August to the 6th and\n0:21\nthat enlisted an increase in about a\n0:24\nmillion people who are going to be\n0:26\ncoming to Lisbon to see the pope which\n0:30\nfor me kind of ignited a bell of oh no\n0:33\nthere's going to be a lot of people\nThe idea\n0:35\neverywhere so the programmer in me\n0:37\nlooked at that and thought okay so how\n0:40\ncan we automate staying away from the\n0:44\nhot spots where you know the pope might\n0:46\nvisit so there might be like a bunch of\n0:48\npeople there I decided to do a little\nExplaining the automation\n0:51\npython automation that essentially helps\n0:54\nme keep track of\n0:56\nwhere the book is going to be and split\n0:59\nit by days so that I know the you know\n1:02\nareas to avoid when I'm walking around\n1:05\nLisbon or you know going to town so what\n1:07\nI did was I copied all the itinerary\nA walkthrough the code\n1:10\nfrom the pope from the official press\n1:11\nrelease and then I threw that into\n1:14\ncharge BT to get some code to structure\n1:17\nthat into a panda's data frame I didn't\n1:19\ndid a little bit of cleaning and I\n1:21\ncertified that all the locations that I\n1:23\nobtained for all the dates and the\n1:25\ntimestamps were correct and associated\n1:28\nwith the correct date and then what I\n1:30\ndid is I used the geopy package to get\n1:34\nthe coordinates the latitude and\nThe final product\n1:36\nlongitude coordinates for each of those\n1:37\nlocations separated by the bay and the\n1:40\ntime when the pope is going to be\n1:42\nvisiting those locations and then I\n1:44\ngenerated this really cool interactive\n1:46\nheat map of the city of Lisbon where you\n1:49\ncan see the spots where the pope is\n1:51\ngoing to be and I have this very simple\n1:53\ninteractive interface where I can click\n1:55\non a date and then see the spots that\n1:57\nthe pope is going to visit that day as\n2:00\nwell as you know little lines connecting\n2:02\nthose spots so that I have enough like a\n2:04\nsmall awareness of the region you know a\n2:07\ntriangulation of the region where the\n2:09\npope is going to be so I know which\n2:11\nplaces to avoid you know whatever which\n2:14\nday of the week when I'm going around\n2:16\ntown I wanted to have this kind of like\n2:18\nholistic view so this ended up being a\n2:21\nvery cool project to do and that's it if\n2:23\nyou like the video don't forget to like\n2:25\nAnd subscribe and see you next time\nLike and subscribe! :) I want you to create a chapters section for this Youtube video with the following format:\n\ud83d\udcda Chapters:\n<double digit:time stamp> - <Concise phrase of a major part of the video>"}, {"role": "assistant", "content": "\ud83d\udcda Chapters:\n\n00:00 - Intro\n00:10 - Pope is coming to Lisbon!\n00:35 - The idea\n00:50 - Explaining the automation\n01:09 - A walkthrough the code\n01:36 - The final product\n02:26 - Like and subscribe! "}]}
